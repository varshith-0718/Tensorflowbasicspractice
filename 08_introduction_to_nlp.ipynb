{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOI+nBXWoY75dglw/JhhGv/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["NLP has the goal of deriving information. out of natural language(could be sequences text or speech)"],"metadata":{"id":"rYXE3cNCxw-V"}},{"cell_type":"code","source":["#using the helper funtions\n","!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py"],"metadata":{"id":"MUyl8SfmyH0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install tensorflow"],"metadata":{"id":"yTaf1C3XySWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"M4PqxPAPyhvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import helper functions that can be used\n","from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys,unzip_data"],"metadata":{"id":"tRy0ieX9yeSJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Getting text dataset\n","The data is from kaggle's introduction to NLP dataset(labeled as disaster or not disaster)"],"metadata":{"id":"bA3-aSYDyqeH"}},{"cell_type":"code","source":["!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip"],"metadata":{"id":"G_0uDrXDzBKC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unzip_data(\"nlp_getting_started.zip\")"],"metadata":{"id":"oaKhlTOBznlD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Visualizing a text dataset\n","\n","to visualize our text samples, we first have to read them in, one way to do so would be to ue Python: https://realpython.com/read-write-files-python/\n","\n","another way is using pandas.\n"],"metadata":{"id":"yN19COiVL7fE"}},{"cell_type":"code","source":["import pandas as pd\n","train_df = pd.read_csv(\"train.csv\")\n","test_df = pd.read_csv(\"test.csv\")\n","train_df.head()"],"metadata":{"id":"yg1G5DmpL_8i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df[\"text\"][0]"],"metadata":{"id":"SZfs01REOaJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#shuffle training dataframe\n","train_df_shuffled = train_df.sample(frac =1 , random_state =42)\n","train_df_shuffled.head()"],"metadata":{"id":"CvY4kl4FOjFH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.head()"],"metadata":{"id":"VMd4hv3KPUO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#how many example of each class?\n","train_df.target.value_counts()"],"metadata":{"id":"NWBHrQQzPeV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#total number of sample\n","len(train_df), len(test_df)"],"metadata":{"id":"GaxJ6Q4fPxft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualize some random training examples\n","import random\n","random_index = random.randint(0,len(train_df)-5) # create random index not higher than the total number of samples\n","for row in train_df_shuffled[[\"text\",\"target\"]][random_index:random_index+5].itertuples():\n","  _,text,target =row\n","  print(f\"Target:{target}\",\"(realdisaster)\" if target > 0 else \"(not real disaster)\")\n","  print(f\"Text:\\n{text}\\n\")\n","  print(\"---\\n\")"],"metadata":{"id":"WlqP8dUNQFNW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#split data into training adn validation sets\n"],"metadata":{"id":"49vtY_9oWWp-"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n"],"metadata":{"id":"EYav9e-NWgNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","# Assuming your data is in the 'train_df_shuffled' DataFrame\n","# 'text' column contains the text and 'target' column contains the labels\n","\n","# Split the dataset into training and validation sets (90% train, 10% validation)\n","train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n","    train_df_shuffled[\"text\"].to_numpy(),  # Features (text)\n","    train_df_shuffled[\"target\"].to_numpy(),  # Labels (target)\n","    test_size=0.1,  # 10% of the data will be used for validation\n","    random_state=42  # For reproducibility\n",")\n","\n","# Check the shape of the resulting splits\n","print(f\"Training sentences: {len(train_sentences)}\")\n","print(f\"Validation sentences: {len(val_sentences)}\")\n","print(f\"Training labels: {len(train_labels)}\")\n","print(f\"Validation labels: {len(val_labels)}\")\n"],"metadata":{"id":"urJo6EG5Y54p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#use traintestsplit to split training data into training and validation sets\n","train_sentences,val_sentences,train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n","                                                                          train_df_shuffled[\"target\"].to_numpy(),\n","                                                                          test_size =0.1,#use 10 percent of training data for validation\n","                                                                          random_state =42)"],"metadata":{"id":"jaS6NuFHW1tf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the lengths\n","len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"],"metadata":{"id":"B61GDDmpYA2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check the first 10 samples\n","train_sentences[:10], train_labels[:10]"],"metadata":{"id":"sPxkq6nUYORN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Converting text into numbers\n","\n","ways to do it:\n","* Tokenization - direct mapping of token( could be word or character ) to number.\n","* Embedding - create a matrix of feature vector for each token (the size of the feature vector can be defined and embeddings can be learned)\n"],"metadata":{"id":"PHxIwIlvYmTO"}},{"cell_type":"markdown","source":["### Text vectorization(tokenization)"],"metadata":{"id":"y-41cOynkkDk"}},{"cell_type":"code","source":["train_sentences[:5]"],"metadata":{"id":"H0DMs_BYopxD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization"],"metadata":{"id":"7Eam_mQ5ovLI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#use the default Text vectorization parameters\n","max_vocab_length = 10000 # set maximum vocabulary size to 10000\n","text_vectorizer = TextVectorization(max_tokens = max_vocab_length,# how many words in the vocabulary (automatically add <OOV>)\n","                                    standardize = \"lower_and_strip_punctuation\",\n","                                    split = \"whitespace\",\n","                                    ngrams = None, #Create groups of n-words?\n","                                    output_mode = \"int\",\n","                                    output_sequence_length = None, #None means it is going to find the longest sequence and pad to that length\n","                                    pad_to_max_tokens = True #pad to the maximum specified in output_sequence_length if its specified or to the longest one if left as default\n","                                    )"],"metadata":{"id":"Xht-MZD9pWvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sentences[0].split()"],"metadata":{"id":"hxHrB3WssgRK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sentences[0]"],"metadata":{"id":"HfNKiSBvs9UT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#find the average number of tokens(words) in the training tweets\n","round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"],"metadata":{"id":"-Q9iSBBDsOM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#setup text vectorization variables\n","max_vocab_length = 10000 # max number of words to have in our vocabulary\n","max_length = 15 #max length of our sequences will be (e.g how many words from a tweet does a model see?)\n","\n","text_vectorizer = TextVectorization(max_tokens = max_vocab_length,\n","                                    output_mode = \"int\",\n","                                    output_sequence_length = max_length)"],"metadata":{"id":"NxHp89SMtd77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fit the text vectorizer to the training text\n","text_vectorizer.adapt(train_sentences)"],"metadata":{"id":"uvaCMyguz7Av"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a sample sentence and tokenize it\n","sample_sentence = \"there's is a flood in my street!\"\n","text_vectorizer([sample_sentence])"],"metadata":{"id":"C380MVqZ0GDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#choose a random sentence from the training dataset and tokenize it\n","random_sentence = random.choice(train_sentences)\n","print(f\"original text: \\n {random_sentence}\\\n","      \\n\\nVectorized version:\")\n","text_vectorizer([random_sentence])"],"metadata":{"id":"u6Czbn520dmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the unique words in the vocabulary\n","words_in_vocab = text_vectorizer.get_vocabulary() #get all of the unique words in the training data\n","top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)"],"metadata":{"id":"FLR7NBBG0SlK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bottom_5_words = words_in_vocab[-5:]\n"],"metadata":{"id":"Rdjym9Zz1oG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n","print(f\"Top 5 most common words: {top_5_words}\")\n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"],"metadata":{"id":"cwRYY-0J1v9T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# creating an embedding using embedding layer\n","\n","the parameters we care most about for our embedding layer:\n","* 'input_dim' = the size of the vocabulary\n","* 'output_dim' = the size of the output embedding vector\n","* 'input_length' = length of the sequences being passed to the embedding layer"],"metadata":{"id":"F97xhuKt2Tnl"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","embedding = layers.Embedding(input_dim = max_vocab_length, #set input shape\n","                             output_dim = 128, #output shape\n","                             embeddings_initializer = \"uniform\",\n","                             input_length = max_length) #how long is each input\n","embedding"],"metadata":{"id":"VxW8Z5qh3DTy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get a random sentnce from the training set\n","random_sentence = random.choice(train_sentences)\n","print(f\"Original text:\\n{random_sentence}\\\n","      \\n\\nEmbedded version:\")\n","\n","# Embed the random sentence (turn it into numerical representation)\n","sample_embed = embedding(text_vectorizer([random_sentence]))\n","sample_embed"],"metadata":{"id":"eKvrCtge3hYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_embed[0][0]"],"metadata":{"id":"daP6Brl04d9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_embed[0][0].shape"],"metadata":{"id":"4RBfd_Nf4hwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_sentence"],"metadata":{"id":"CKamwx_a4hxB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["road map to machine learning https://scikit-learn.org/1.3/tutorial/machine_learning_map/"],"metadata":{"id":"AwE9qqx36VPG"}},{"cell_type":"markdown","source":["#Modelling a text dataset\n","\n","* Model 0 : Naive Bayes(baseline)\n","* Model 1 : Feed-forward Neural Network(dense model)\n","* Model 2 : LSTM model(RNN)\n","* Model 3 : GRU model (RNN)\n","* Model 4 : Bidirectional-LSTM model(RNN)\n","* Model 5 : 1D convolutional Neural Network(CNN)\n","* Model 6 : Tensorflow Hub Pretrained Feature Extractor (using transfer learning for NLP)\n","* Model 7 : Same as model 6 with 10% of training data.\n","\n"],"metadata":{"id":"MWGvof7F5US1"}},{"cell_type":"markdown","source":["#Model 0 : Getting a baseline\n","\n","To create our baseline, we'll use sklearn's multinomial naive bayes using the TF-IDF formula to convert words to numbers."],"metadata":{"id":"g8P0JoHm7QX5"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","# create tokenization and modelling pipeline\n","model_0 = Pipeline([\n","    (\"tfidf\",TfidfVectorizer()), #convert words to numbers using tfidf\n","    (\"clf\",MultinomialNB()) #model the text\n","])\n","#Fit the pipeline to the training data\n","model_0.fit(train_sentences,train_labels)"],"metadata":{"id":"mkowgR_JL_iH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the baseline model\n","baseline_score = model_0.score(val_sentences,val_labels)\n","print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"],"metadata":{"id":"vF0VreOUOOkn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df.target.value_counts()"],"metadata":{"id":"i18stf9lOoWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["baseline_preds = model_0.predict(val_sentences)\n","baseline_preds[:20]"],"metadata":{"id":"YXeyvjDZOuwz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating an evaluation function for our model experiments\n","\n","* Accuracy\n","* Precision\n","* Recall\n","* F1-score\n"],"metadata":{"id":"sWrQa9OkO-I3"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n","\n","def calculate_results(y_true,y_pred):\n","  \"\"\"\n","  Calculates model accuracy,precision,recall and f1 score of a binary classification model\n","  \"\"\"\n","  #calculate model accuracy\n","  model_accuracy = accuracy_score(y_true,y_pred)*100\n","  # calculate model precision, recall and F1-score\n","  model_precision,model_recall,model_f1,_ = precision_recall_fscore_support(y_true,y_pred,average = \"weighted\")\n","  model_results = {\"accuracy\":model_accuracy,\n","                   \"precision\":model_precision,\n","                   \"recall\":model_recall,\n","                   \"f1\":model_f1}\n","  return model_results"],"metadata":{"id":"jK3Rhi8kPC_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get baseline results\n","baseline_results = calculate_results(y_true = val_labels,\n","                                     y_pred = baseline_preds)\n","baseline_results"],"metadata":{"id":"JIwoeqHYTrd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model1 : A simple dense model\n"],"metadata":{"id":"sSNxH-QwTdAB"}},{"cell_type":"code","source":["# create a directory to save Tensorboard logs\n","SAVE_DIR = \"model_logs\""],"metadata":{"id":"yNv9obIgWchb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import layers\n","inputs = layers.Input(shape = (1,),dtype = tf.string)# inputs are 1-dimensional strings\n","x = text_vectorizer(inputs) # turn the input text into numbers\n","x = embedding(x)# create an embedding of the numberized inputs\n","outputs = layers.Dense(1,activation = \"sigmoid\")(x) #create the output layer, want binary outputs so use sigmoid activation\n","model_1 = tf.keras.Model(inputs,outputs,name = \"model_1_dense\")\n"],"metadata":{"id":"pOREiIaqZ3VQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_1.summary()"],"metadata":{"id":"x-nykwtMcuWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import layers\n","inputs = layers.Input(shape = (1,),dtype = tf.string)# inputs are 1-dimensional strings\n","x = text_vectorizer(inputs) # turn the input text into numbers\n","x = embedding(x)# create an embedding of the numberized inputs\n","x = layers.GlobalAveragePooling1D()(x) # condense the feature vecotr for eact token to get single vector\n","outputs = layers.Dense(1,activation = \"sigmoid\")(x) #create the output layer, want binary outputs so use sigmoid activation\n","model_1 = tf.keras.Model(inputs,outputs,name = \"model_1_dense\")"],"metadata":{"id":"bYK2NkAfcn7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_1.summary()"],"metadata":{"id":"BnlwvdbSa1NA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#compile model\n","model_1.compile(loss = \"binary_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])"],"metadata":{"id":"83dx-n0qbGhr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fit the model\n","model_1_history = model_1.fit(x = train_sentences,\n","                                 y = train_labels,\n","                                 epochs = 5,\n","                                 validation_data = (val_sentences,val_labels),\n","                              callbacks= [create_tensorboard_callback(dir_name = SAVE_DIR,\n","                                          experiment_name = \"model_1_dense\")])"],"metadata":{"id":"czN4mplqbJeU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the results\n","model_1.evaluate(val_sentences,val_labels)"],"metadata":{"id":"QFMSq-5KdlCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#make some predictions and evaluate those\n","model_1_pred_probs = model_1.predict(val_sentences)\n","model_1_pred_probs.shape, model_1_pred_probs[0]"],"metadata":{"id":"6HloqbUZdp5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#convert model prediction probabilities to label format\n","model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n","model_1_preds[:10]"],"metadata":{"id":"NtdKKWINfb0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#calculate model1 results\n","model_1_results = calculate_results(y_true = val_labels,\n","                                    y_pred = model_1_preds)\n","model_1_results"],"metadata":{"id":"hYuz--d_f1nj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["baseline_results"],"metadata":{"id":"goX6yuzsgAF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","np.array(list(baseline_results.values())) > np.array(list(model_1_results.values()))"],"metadata":{"id":"mRC6pue-gG0c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#visualizing the model embedding"],"metadata":{"id":"tF6B_8CUr8UC"}},{"cell_type":"code","source":["#Get the vocabulary from the text vectorization layer\n","words_in_vocab"],"metadata":{"id":"ZTOiMpB9sxyv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(words_in_vocab)"],"metadata":{"id":"SjBz58ins-co"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the vocabulary from text vectorization layer\n","words_in_vocab = text_vectorizer.get_vocabulary()\n","words_in_vocab[:10],len(words_in_vocab)"],"metadata":{"id":"FqiOLMg1to5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Model 1 summary\n","model_1.summary()"],"metadata":{"id":"lSLjV-0xuFKq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Get the weight matrix of embedding layer\n","#(these are the numerical representations of each token in our training data, which have been learned for f5 epochs)\n","embed_weights = model_1.get_layer(\"embedding\").get_weights()\n","embed_weights\n"],"metadata":{"id":"2i6-cpa8umyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(embed_weights[0].shape)\n"],"metadata":{"id":"zjUFxBqJ10dj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize the embedding matrix which learned to represent our tokens using projector\n","\n","study : https://jalammar.github.io/illustrated-word2vec/\n","\n","work environment: https://projector.tensorflow.org/"],"metadata":{"id":"caGYZ14p2Jao"}},{"cell_type":"code","source":["weights = model_1.get_layer(\"embedding\").get_weights()[0]\n","vocab = text_vectorizer.get_vocabulary()"],"metadata":{"id":"OK2_idCf37lZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create embedding files (we got this from TensorFlow's word embeddings documentation)\n","import io\n","out_v = io.open(\"vectors.tsv\",\"w\",encoding = \"utf-8\")\n","out_m = io.open(\"metadata.tsv\",\"w\",encoding = \"utf-8\")\n","for index, word in enumerate(vocab):\n","  if index == 0:\n","    continue # skip 0 , it's padding\n","  vec = weights[index]\n","  out_v.write('\\t'.join([str(x) for x in vec])+\"\\n\")\n","  out_m.write(word+\"\\n\")\n","out_v.close()\n","out_m.close()"],"metadata":{"id":"QoXmxoQh2yfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create embedding files (we got this from TensorFlow's word embeddings documentation)\n","import io\n","out_v = io.open(\"vectors.tsv\",\"w\",encoding = \"utf-8\")\n","out_m = io.open(\"metadata.tsv\",\"w\",encoding = \"utf-8\")\n","for index, word in enumerate(words_in_vocab):\n","  if index == 0:\n","    continue # skip 0 , it's padding\n","  # the error was caused because the index was larger than the shape of the embed_weights\n","  # we need to ensure the index is less than the length of weights\n","  if index < embed_weights[0].shape[0]:\n","    vec = embed_weights[0][index]\n","    out_v.write('\\t'.join([str(x) for x in vec])+\"\\n\")\n","    out_m.write(word+\"\\n\")\n","out_v.close()\n","out_m.close()"],"metadata":{"id":"EXPME94X4kTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#download files from colab to upload to projector\n","try:\n","  from google.colab import files\n","  files.download('vectors.tsv')\n","  files.download('metadata.tsv')\n","except Exception:\n","  pass"],"metadata":{"id":"oZ4Yh0dl5nSo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recurrent Neural Networks (RNN's)\n","\n","RNN's are useful for sequence data.\n","it is to use the representation of previous input to aid the representation of\n","a later input."],"metadata":{"id":"Mc2vjYGl6MC7"}},{"cell_type":"markdown","source":["### Model 2 : LSTM(Long Short Term Memory)\n","\n"],"metadata":{"id":"w9PnNeJW8k-v"}},{"cell_type":"code","source":["#Create an LSTM model\n","from tensorflow.keras import layers\n","inputs = layers.Input(shape = (1,),dtype = tf.string)\n","x = text_vectorizer(inputs)\n","x = embedding(x)\n","print(x.shape)\n","x = layers.LSTM(64,return_sequences = True)(x)# when you're stacking RNN cells together, you need to return_sequence\n","print(x.shape)\n","x = layers.LSTM(64)(x)\n","print(x.shape)\n","x = layers.Dense(64,activation = \"relu\")(x)\n","outputs = layers.Dense(1,activation = \"sigmoid\")(x)\n","model_2 = tf.keras.Model(inputs,outputs,name = \"model_2_LSTM\")"],"metadata":{"id":"jHQtoAJn8k_j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2.summary()"],"metadata":{"id":"Gipyb49VyBvM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#compile the model\n","model_2.compile(loss = \"binary_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])"],"metadata":{"id":"LQJIQHb42cCp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fit the model\n","model_2_history = model_2.fit(train_sentences,\n","                              train_labels,\n","                              epochs = 5,\n","                              validation_data = (val_sentences, val_labels),\n","                              callbacks = [create_tensorboard_callback(SAVE_DIR,\"model_2_LSTM\")])"],"metadata":{"id":"Ix8P3W9c2jZI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Make predictions with LSTM model\n","model_2_pred_probs = model_2.predict(val_sentences)\n","model_2_pred_probs[:10]"],"metadata":{"id":"F_LOlOzl3fLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#covert model 2 pred probs to labels\n","model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n","model_2_preds[:10]"],"metadata":{"id":"kr5T9DE13nP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_labels"],"metadata":{"id":"ulucJQCW3s-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calculate model 2 results\n","model_2_results = calculate_results(y_true = val_labels,\n","                                    y_pred = model_2_preds)\n","model_2_results"],"metadata":{"id":"tSu6B37f3wfm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["baseline_results"],"metadata":{"id":"iYff1QRH39Xs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 3 : GRU\n","\n","Another popular and effective RNN cell is the Gated Recurrent Unit (GRU) it has less parameters than LSTM."],"metadata":{"id":"XC-TF4Sa4Sc5"}},{"cell_type":"code","source":["#Create an GRU model\n","from tensorflow.keras import layers\n","inputs = layers.Input(shape = (1,),dtype = tf.string)\n","x = text_vectorizer(inputs)\n","x = embedding(x)\n","print(x.shape)\n","x = layers.GRU(64,return_sequences = True)(x)# when you're stacking RNN cells on top of each other, you need to return_sequence\n","print(x.shape)\n","x = layers.GRU(64)(x)\n","print(x.shape)\n","x = layers.Dense(64,activation = \"relu\")(x)\n","outputs = layers.Dense(1,activation = \"sigmoid\")(x)\n","model_3 = tf.keras.Model(inputs,outputs,name = \"model_3_GRU\")"],"metadata":{"id":"jy2EnxLQ4roV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_3.compile(loss = \"binary_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])"],"metadata":{"id":"ms2X4QrK52lI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fit the model\n","model_3_history = model_3.fit(train_sentences,\n","                              train_labels,\n","                              epochs = 5,\n","                              validation_data = (val_sentences, val_labels),\n","                              callbacks = [create_tensorboard_callback(SAVE_DIR,\"model_3_GRU\")])"],"metadata":{"id":"OfvLThhv59UE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_3_pred_probs = model_3.predict(val_sentences)\n","model_3_pred_probs[:10]"],"metadata":{"id":"f9WWbjbS6F3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#convert model 3 predprobs to labels\n","model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n","model_3_preds[:10]"],"metadata":{"id":"pDOdjL_h6JJO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate model 3 results\n","model_3_results = calculate_results(y_true = val_labels,\n","                                    y_pred = model_3_preds)\n","model_3_results"],"metadata":{"id":"BKymSPss6Rv_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model 4: Bidirectional RNN\n","\n","Normal RNN'S go from left to right\n","but this model go from left to right and right to left\n"],"metadata":{"id":"HCRfYN7k6xYI"}},{"cell_type":"code","source":["#Build the model\n","from tensorflow.keras import layers\n","inputs = layers.Input(shape = (1,), dtype = \"string\")\n","x = text_vectorizer(inputs)\n","x = embedding(x)\n","x = layers.Bidirectional(layers.LSTM(64,return_sequences = True))(x)\n","print(x.shape)\n","x = layers.Bidirectional(layers.GRU(64))(x)\n","print(x.shape)\n","x = layers.Dense(64,activation = \"relu\")(x)\n","outputs = layers.Dense(1,activation = \"sigmoid\")(x)\n","model_4 = tf.keras.Model(inputs,outputs,name = \"model_4_Bidirectional\")"],"metadata":{"id":"8jWsz2Js7UuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_4.summary()"],"metadata":{"id":"1YSqN7ma8L4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#compile the model\n","model_4.compile(loss = \"binary_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])"],"metadata":{"id":"-CPykt2m8oW6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fit the model\n","model_4_history = model_4.fit(train_sentences,\n","                              train_labels,\n","                              epochs = 5,\n","                              validation_data = (val_sentences,val_labels),\n","                              callbacks = [create_tensorboard_callback(SAVE_DIR,\"model_4_Bidirectional\")])"],"metadata":{"id":"sFS2YTpO8oYL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#predict the model\n","model_4_pred_probs = model_4.predict(val_sentences)\n","model_4_pred_probs[:10]"],"metadata":{"id":"RQ7H7XG68wh6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#squeeze the the probs\n","model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n","model_4_preds[:10]"],"metadata":{"id":"5t8X3sOp858g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#results\n","model_4_results = calculate_results(y_true = val_labels,\n","                                    y_pred = model_4_preds)\n","model_4_results"],"metadata":{"id":"tBMg4PL69Oih"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Convolutional Neural Networks for Text(and other types of sequences)\n","we've used CNNs for images are typically 2D but now it is in 1D"],"metadata":{"id":"UXaX_YdCHcPL"}},{"cell_type":"markdown","source":["#Model 5 : Conv1D\n"],"metadata":{"id":"qkTysjo0Hszp"}},{"cell_type":"code","source":["# Test ourt our embedding layer, conv1D layer and max pooling\n","from tensorflow.keras import layers\n","embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) #turn target sequence into embedding\n","conv_1d = layers.Conv1D(filters = 32 ,\n","                        kernel_size = 5,\n","                        activation = \"relu\",\n","                        padding = \"valid\")\n","conv_1d_output = conv_1d(embedding_test)# pass test embedding through conv1d layer\n","max_pool = layers.GlobalMaxPool1D()\n","max_pool_output = max_pool(conv_1d_output)# \"get the most important feature or get feature with highest value\"\n","embedding_test.shape, conv_1d_output.shape, max_pool_output.shape\n"],"metadata":{"id":"fOnGxcqfIT3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_test"],"metadata":{"id":"okf6KyoWLb45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv_1d_output"],"metadata":{"id":"fhsjAI_lML0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_pool_output"],"metadata":{"id":"gasGMjGKMPOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import layers\n","inputs = layers.Input(shape= (1,),dtype = tf.string)\n","x = text_vectorizer(inputs)\n","x = embedding(x)\n","x = layers.Conv1D(filters = 64,kernel_size = 5 , activation = \"relu\",padding = \"valid\")(x)\n","x  = layers.GlobalMaxPool1D()(x)\n","outputs = layers.Dense(1,activation = \"sigmoid\")(x)\n","model_5 = tf.keras.Model(inputs,outputs,name = \"model_5_Conv1D\")"],"metadata":{"id":"TMJhq6EIMYpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#compile the model\n","model_5.compile(loss = \"binary_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])"],"metadata":{"id":"dfKrtCBhNSHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fit the model\n","model_5_history = model_5.fit(train_sentences,\n","                              train_labels,\n","                              epochs = 5,\n","                              validation_data = (val_sentences,val_labels),\n","                              callbacks = [create_tensorboard_callback(SAVE_DIR,\"model_5_Conv1D\")])"],"metadata":{"id":"tOGyp88gNWJf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Predict\n","model_5_pred_probs = model_5.predict(val_sentences)\n","model_5_pred_probs[:10]"],"metadata":{"id":"wQhLqv_jNtMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#results\n","model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n","model_5_preds[:10]"],"metadata":{"id":"xOXaSdMmNwa_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from helper_functions import calculate_results"],"metadata":{"id":"BVrWwx6TOOJM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#results\n","model_5_results = calculate_results(y_true = val_labels,\n","                                    y_pred = model_5_preds)\n","model_5_results\n"],"metadata":{"id":"Vs8mtSMdOFm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model 6: TensorFlow Hub Pretrained Sentence Encoder\n"],"metadata":{"id":"sD1wMlNdSWgp"}},{"cell_type":"code","source":["import tensorflow_hub as hub\n","\n","embed = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\")\n","embed_samples = embed([\n","    sample_sentence,\n","    \"I am a sentence for which I would like to get its embedding\"])\n","\n","print(embed_samples[0][:50])\n","\n","# The following are example embedding output of 512 dimensions per sentence\n","# Embedding for: The quick brown fox jumps over the lazy dog.\n","# [-0.03133016 -0.06338634 -0.01607501, ...]\n","# Embedding for: I am a sentence for which I would like to get its embedding.\n","# [0.05080863 -0.0165243   0.01573782, ...]\n"],"metadata":{"id":"qIRsmXFiUYic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_samples"],"metadata":{"id":"qmb82mkAVJxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Universal Sentence Encoder (USE) from Kaggle Models\n","sentence_encoder_layer = hub.KerasLayer(\n","    \"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\",\n","    dtype=tf.string,\n","    trainable=False,\n","    name=\"USE_Layer\"\n",")"],"metadata":{"id":"KBSiaEOCV7Hz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras import layers, Model\n","\n","\n","# Define the Text Classification Model\n","def build_text_classifier(num_classes=2):\n","    \"\"\"\n","    Creates a text classification model using USE as an embedding layer.\n","\n","    Args:\n","        num_classes (int): Number of output classes (default: 2 for binary classification).\n","\n","    Returns:\n","        model (tf.keras.Model): Compiled text classification model.\n","    \"\"\"\n","\n","    # Input layer (text data)\n","    text_input = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n","\n","    # Wrap USE inside Lambda with explicit output shape\n","    embedding = layers.Lambda(lambda x: sentence_encoder_layer(x), output_shape=(512,), name=\"USE_Wrapper\")(text_input)\n","\n","    # Fully connected layers\n","    x = layers.Dense(128, activation=\"relu\")(embedding)\n","    # Output layer\n","    if num_classes == 2:\n","        output = layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\")(x)  # Binary classification\n","        loss = \"binary_crossentropy\"\n","    else:\n","        output = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(x)  # Multi-class classification\n","        loss = \"sparse_categorical_crossentropy\"\n","\n","    # Create and compile model\n","    model = Model(inputs=text_input, outputs=output, name=\"text_classifier\")\n","    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","\n","    return model\n","\n","# Create the text classification model\n","model_6 = build_text_classifier(num_classes=2)\n","\n","# Model summary\n","model_6.summary()\n"],"metadata":{"id":"tG2PJM3mO_Ue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_6.compile(loss = \"binary_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])"],"metadata":{"id":"LyuXIkH0Xuxl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#transfer learning is not executing"],"metadata":{"id":"UWTwQY86aziB"}},{"cell_type":"markdown","source":["#model 7: using 10% data\n"],"metadata":{"id":"OcesEndZfXAU"}},{"cell_type":"code","source":["train_10_percent = train_df_shuffled[[\"text\",\"target\"]].sample(frac = 0.1,random_state = 42)\n","train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n","train_labels_10_percent = train_10_percent[\"target\"].to_list()"],"metadata":{"id":"QtqdeSe5fdGH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check the number of targets in our subset of data\n","train_10_percent[\"target\"].value_counts()"],"metadata":{"id":"TWhwQy-BgHV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#results in dataframe\n","all_model_results = pd.DataFrame({\"0_baseline\":baseline_results,\n","                                 \"1_simple_dense\":model_1_results,\n","                                 \"2_lstm\": model_2_results,\n","                                 \"3_GRU\": model_3_results,\n","                                 \"4_bidirectional\": model_4_results,\n","                                 \"5_conv1d\": model_5_results})\n","all_model_results = all_model_results.transpose()\n"],"metadata":{"id":"ikiNdYD16Jiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_model_results"],"metadata":{"id":"YNZ5MrhS__Tf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#reduce the accuracy to the same scale as other metrics\n","all_model_results[\"accuracy\"]= all_model_results[\"accuracy\"]*100"],"metadata":{"id":"meFUojrA7CFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot and compare all ot the model results\n","all_model_results.plot(kind = \"bar\",figsize=(10,7)).legend(bbox_to_anchor=(1.0,1.0));"],"metadata":{"id":"GBh2Y-YX7XfT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#sort model results by f1-score\n","all_model_results.sort_values(\"f1\",ascending = False)[\"f1\"].plot(kind=\"bar\",figsize=(10,7));"],"metadata":{"id":"w9pAuqiU7rFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install google-cloud-storage\n"],"metadata":{"id":"F-iEaOKf-JCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from getpass import getpass\n","\n","# Enter your GitHub username\n","github_username = \"varshith2415\"\n","\n","# Enter your GitHub token (you can paste it here, or use getpass for security)\n","github_token = getpass(\"Enter your GitHub token: \")\n","\n","# Store authentication details securely\n","os.environ[\"GITHUB_USERNAME\"] = github_username\n","os.environ[\"GITHUB_TOKEN\"] = github_token\n","\n"],"metadata":{"id":"o6K_xgS1BYoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get install git  # Install Git if not already installed\n","\n","# Set up Git configuration (with your GitHub username and email)\n","!git config --global user.name \"varshith2415\"\n","!git config --global user.email \"varshithreddyraavi@gmail.com\"  # Replace with your email\n"],"metadata":{"id":"zyTSJpcND6ho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/varshith2415/tensorflow-logs.git\n"],"metadata":{"id":"IwM0dtagEbOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming the model_logs directory is in your current working directory in Colab\n","!cp -r /content/model_logs /content/tensorflow-logs/\n"],"metadata":{"id":"xH9PIblpEz_f"},"execution_count":null,"outputs":[]}]}