{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMMx+HQ31PMsdV91JXtAq6y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wmcjgwtDRss2"},"outputs":[],"source":["#using the helper funtions\n","!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py"]},{"cell_type":"code","source":["#using the helper functions\n","from helper_functions import unzip_data,create_tensorboard_callback,plot_loss_curves,compare_historys,walk_through_dir"],"metadata":{"id":"VsIU3NDeTf8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip"],"metadata":{"id":"lo1Y9i7B_DYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unzip_data(\"10_food_classes_10_percent.zip\")"],"metadata":{"id":"wzpDVmL__PXh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#checkingout how many imags and subdirectories\n","walk_through_dir(\"10_food_classes_10_percent\")"],"metadata":{"id":"jLN9ELVq_Zgo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create training and test directory paths\n","train_dir = \"10_food_classes_10_percent/train\"\n","test_dir = \"10_food_classes_10_percent/test\""],"metadata":{"id":"EEOcxPlU_qDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","IMG_SIZE = (224,224)\n","BATCH_SIZE =32\n","train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory = train_dir,\n","                                                                            image_size = IMG_SIZE,\n","                                                                            label_mode = \"categorical\",\n","                                                                            batch_size = BATCH_SIZE)\n","\n","test_data = tf.keras.preprocessing.image_dataset_from_directory(directory = test_dir,\n","                                                                 image_size = IMG_SIZE,\n","                                                                 label_mode = \"categorical\",\n","                                                                 batch_size = BATCH_SIZE)"],"metadata":{"id":"V1JHUANr_zJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_10_percent"],"metadata":{"id":"q7iIg64ZA9mZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset shapes and visualization or conversion to numpy"],"metadata":{"id":"2oECbE-eHnht"}},{"cell_type":"code","source":["batch_shapes = [spec.shape for spec in train_data_10_percent.element_spec]\n","print(batch_shapes)\n"],"metadata":{"id":"UZMti3Y5EwbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for image_batch, label_batch in train_data_10_percent.take(1):\n","    print(\"Actual Image Batch Shape:\", image_batch.shape)  # Should be (32, 224, 224, 3) if batch_size=32\n","    print(\"Actual Label Batch Shape:\", label_batch.shape)  # Should be (32, 10)\n"],"metadata":{"id":"G3c2gtlfGase"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","# Assume you have a dataset\n","dataset = ...  # Your PrefetchDataset or BatchDataset\n","\n","# Get element_spec\n","input_spec, label_spec = train_data_10_percent.element_spec\n","\n","# Generate random tensors with the correct shape (replace None with a fixed batch size)\n","\n","\n","input_data = tf.random.uniform(shape=(BATCH_SIZE, *input_spec.shape[1:]), dtype=input_spec.dtype)\n","label_data = tf.random.uniform(shape=(BATCH_SIZE, *label_spec.shape[1:]), dtype=label_spec.dtype)\n","\n","print(\"Generated Input Data Shape:\", input_data.shape)\n","print(\"Generated Label Data Shape:\", label_data.shape)\n"],"metadata":{"id":"bpMfQ-rjGugS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_dataset = tf.data.Dataset.from_tensor_slices((input_data, label_data)).batch(BATCH_SIZE)\n","\n","# Check the shapes of the first batch\n","for batch in new_dataset.take(1):\n","    print([tensor.shape for tensor in batch])\n"],"metadata":{"id":"UYjKrxGrHMC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_np = input_data.numpy()\n","label_np = label_data.numpy()\n","print(\"NumPy Input Shape:\", input_np.shape)\n","print(\"NumPy Label Shape:\", label_np.shape)\n"],"metadata":{"id":"2T0x2L31Hgmz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Continuing from there."],"metadata":{"id":"L5p6YOBqIS7x"}},{"cell_type":"code","source":["train_data_10_percent.class_names"],"metadata":{"id":"agJNstGXH0Nq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Get one batch\n","image_batch, label_batch = next(iter(train_data_10_percent))\n","\n","# Check min and max pixel values\n","print(\"Min pixel value:\", np.min(image_batch.numpy()))\n","print(\"Max pixel value:\", np.max(image_batch.numpy()))\n"],"metadata":{"id":"wGvr6SLtJeeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#see an example of a batch of data\n","for images,labels in train_data_10_percent.take(1):\n","  print(images,labels)"],"metadata":{"id":"u1nmDFcpIdnP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#model 0 : building a transfer learning model using the keras Funcitonal API\n","\n","Functional API provides more flexibility in building models."],"metadata":{"id":"SCm7llIBKAEN"}},{"cell_type":"code","source":["#1. create base model with tf.keras.applications.\n","\n","base_model = tf.keras.applications.EfficientNetB0(include_top = False)\n","\n","#2. Freeze the base model( so the underlying pre-trained patterns aren't updated)\n","base_model.trainable = False\n","\n","#3. create inputs into our model\n","inputs = tf.keras.layers.Input(shape = (224,224,3), name = \"input_layer\")\n","\n","#4. If using a model like ResNet50V2 we need to normalize inputs (efficient don't need as it has already builtin if using from tf.keras)\n","#x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n","\n","# 5. Pass the inputs to the base_model\n","x = base_model(inputs)\n","print(f\"Shape after passing inputs through base model: {x.shape}\")\n","\n","#6. Average pool the ouputs of the base model (aggregate all the most important information, reduce number of computations)\n","x = tf.keras.layers.GlobalAveragePooling2D(name= \"global_average_pooling_layer\")(x)\n","print(f\"Shape after Global Average Pooling: {x.shape}\")\n","\n","#7. Create the output activation layer\n","outputs = tf.keras.layers.Dense(10, activation = \"softmax\", name = \"output_layer\")(x)\n","print(f\"Shape after output layer: {outputs.shape}\")\n","\n","#8. combine the inputs with the outputs into a model.\n","model_0 = tf.keras.Model(inputs, outputs)\n","\n","#9. compile the model\n","model_0.compile(loss = \"categorical_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])\n","\n","#10. fit the model and sae its history\n","history_10_percent = model_0.fit(train_data_10_percent,\n","                        epochs = 5,\n","                        steps_per_epoch = len(train_data_10_percent),\n","                        validation_data = test_data,\n","                                 validation_steps = int(0.25 * len(test_data)),\n","                                 callbacks = [create_tensorboard_callback(dir_name = \"transfer_learning\",\n","                                                                           experiment_name = \"efficientnetb0_10_percent_data\")]\n","                                 )\n"],"metadata":{"id":"Wkndr-TDLIdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#evaluate on the the full test dataset\n","model_0.evaluate(test_data)"],"metadata":{"id":"Z4FUzEi_PmAA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check the layers in our base model.\n","for layer_number, layer in enumerate(base_model.layers):\n","  print(layer_number,layer.name)"],"metadata":{"id":"G2rbedegPwEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model.summary()\n"],"metadata":{"id":"sZXPqCnxQIe7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_0.summary()"],"metadata":{"id":"TN4uXoOqQajL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot loss curves\n","plot_loss_curves(history_10_percent)"],"metadata":{"id":"PdwtoD2mQly7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Getting a feature vector from a trained model\n","\n","demonstrating the global average pooling 2D layer...\n","\n","we have a tensor after our model goes through 'base_model' of shape(None, 7,7,1280)\n","\n","But when it passes through the GlobalAveragePooling2D, it turns into (None, 1280)\n","\n","try (1,4,4,3)"],"metadata":{"id":"_dbJ6LQzQ3H0"}},{"cell_type":"code","source":["# Define the input shape\n","input_shape = (1,4,4,3)\n","\n","# Create a random tensor\n","tf.random.set_seed(42)\n","input_tensor = tf.random.normal(input_shape)\n","print(f\"Random input tensor : {input_tensor}\")\n","input_tensor.shape\n","\n"],"metadata":{"id":"Rb-TQHlgSCSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pass the ranodm tensor through a global average pooling 2D layer\n","global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\n","print(f\"2D Global Average Pooled random tensor: {global_average_pooled_tensor}\")"],"metadata":{"id":"DI_h41gfvCf7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_average_pooled_tensor.shape"],"metadata":{"id":"o2fSPUUxSjhy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's replicate the GlobalAveragePool2D layer\n","tf.reduce_mean(input_tensor, axis = [1,2])"],"metadata":{"id":"JR6Sr_0IwUm3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Running a series of transfer learning experiments\n","\n","1. model_1 : use feature extraction transfer learning with 1% training data wth data augmentation\n","2. model_2 : use feature extraction transfer learning with 10% training datawith data augmentation\n","3. model_3 : use fine-tuning transfer learning on 10% of the training data with data augmentation\n","4. model_4 : use fine-tuning transfer learning on 100% of the training data with data augmentation."],"metadata":{"id":"1Kc_2mYvxtUk"}},{"cell_type":"markdown","source":["#Getting and preprocessing data for model_1"],"metadata":{"id":"WHR-SgpIxNMu"}},{"cell_type":"code","source":["#downloading and unziping the data\n","!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n","\n","#unzip the data using the helper_function.py module\n","unzip_data(\"10_food_classes_1_percent.zip\")"],"metadata":{"id":"alwrqPAEyKaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create training and test dirs\n","train_dir_1_percent = \"10_food_classes_1_percent/train\"\n","test_dir_1_percent = \"10_food_classes_1_percent/test\""],"metadata":{"id":"NKCrlDk8yfe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#How many images are we working with\n","walk_through_dir(\"10_food_classes_1_percent\")"],"metadata":{"id":"mG1AQi1syntm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#setup data loaders\n","IMG_SIZE = (224,224)\n","BATCH_SIZE = 32\n","train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(directory = train_dir_1_percent,\n","                                                                            image_size = IMG_SIZE,\n","                                                                            label_mode = \"categorical\",\n","                                                                            batch_size = BATCH_SIZE)\n","\n","test_data = tf.keras.preprocessing.image_dataset_from_directory(directory = test_dir_1_percent,\n","                                                                 image_size = IMG_SIZE,\n","                                                                 label_mode = \"categorical\",\n","                                                                 batch_size = BATCH_SIZE)"],"metadata":{"id":"hN2StbDzyx22"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Adding data augmentation into the model.\n","\n","To add data augmentation to our model, we can use the layers inside:\n","\n","* tf.keras.layers.experimental.preprocessing()"],"metadata":{"id":"U_GpvXuHzdgD"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# Create data augmentation stage with horizontal flipping, rotations, zooms, etc\n","data_augmentation = keras.Sequential([\n","    tf.keras.layers.RandomFlip(\"horizontal\"), # Corrected typo: \"horizonatal\" to \"horizontal\"\n","    tf.keras.layers.RandomRotation(0.2),\n","    tf.keras.layers.RandomZoom(0.2),\n","    tf.keras.layers.RandomHeight(0.2),\n","    tf.keras.layers.RandomWidth(0.2),\n","    # tf.keras.layers.Rescaling(1./255) #Keep this for models like ResNet50V2 but EfficientNet already has a built-in preprocessing.\n","], name=\"data_augmentation\")"],"metadata":{"id":"RLyq0AXFzp8p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#use above or this code instead\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomHeight, RandomWidth\n","# Create data augmentation stage with horizontal flipping, rotations, zooms, etc\n","data_augmentation = keras.Sequential([\n","    RandomFlip(\"horizontal\"), # Corrected typo: \"horizonatal\" to \"horizontal\"\n","    RandomRotation(0.2),\n","    RandomZoom(0.2),\n","    RandomHeight(0.2),\n","    RandomWidth(0.2),\n","    # Rescaling(1./255) #Keep this for models like ResNet50V2 but EfficientNet already has a built-in preprocessing.\n","], name=\"data_augmentation\")"],"metadata":{"id":"75Ey2xjM3RP1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize our data augmentation layer"],"metadata":{"id":"sFiNaPVU3uhx"}},{"cell_type":"code","source":["#view a random image an compare it to ts augmented version\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import os\n","import random"],"metadata":{"id":"G15CXzqi4i3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_class = random.choice(train_data_1_percent.class_names)"],"metadata":{"id":"iiOv-OVY8mmD"},"execution_count":null,"outputs":[]}]}